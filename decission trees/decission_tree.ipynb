{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Trees in Machine Learning\n",
        "\n",
        "## 1. What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks. In classification, the Decision Tree splits the data into different classes based on the features. It works by recursively dividing the dataset into subsets using decision rules at each node. At each node, the algorithm selects a feature that best splits the data, using criteria like Gini Impurity or Information Gain. The process continues until the tree is deep enough or all data points are perfectly classified.\n",
        "\n",
        "\n",
        "## 2. Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "\n",
        "- **Gini Impurity** measures the \"impurity\" of a dataset. It ranges from 0 to 1, where 0 indicates perfect purity (all samples belong to the same class) and 1 means maximum impurity (equal distribution of classes).\n",
        "\n",
        "- **Entropy** is another measure of impurity, which is based on the concept of information theory. It quantifies the disorder or randomness in the dataset. Like Gini, entropy also ranges from 0 to 1, where 0 represents perfect classification.\n",
        "\n",
        "Both measures influence the splits in the tree—lower values indicate better splits. The algorithm chooses the feature that results in the lowest impurity after the split.\n",
        "\n",
        "\n",
        "## 3. What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "- **Pre-Pruning** involves stopping the tree from growing too deep during training by imposing limits like `max_depth` or `min_samples_split`. This prevents overfitting by not allowing the tree to model too much noise from the training data. A practical advantage of pre-pruning is that it helps control the complexity of the tree early on, making it faster to train.\n",
        "\n",
        "- **Post-Pruning** is the process of trimming branches after the tree has been fully grown. The idea is to remove parts of the tree that have little predictive power. A practical advantage of post-pruning is that it allows the tree to grow fully and capture complex patterns, then fine-tunes the model to prevent overfitting.\n",
        "\n",
        "\n",
        "## 4. What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "Information Gain measures how much uncertainty (or entropy) is reduced when the data is split on a particular feature. It calculates the difference between the entropy of the dataset before and after the split. The feature with the highest Information Gain is selected for the split because it provides the most \"information\" in terms of reducing uncertainty. This helps the tree make decisions that improve classification accuracy.\n",
        "\n",
        "\n",
        "## 5. What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "- **Real-World Applications**:\n",
        "  - **Healthcare**: Decision Trees are used for diagnosing diseases based on patient data.\n",
        "  - **Finance**: They help in credit scoring and risk assessment.\n",
        "  - **Marketing**: Decision Trees can predict customer behavior and preferences.\n",
        "\n",
        "- **Advantages**:\n",
        "  - Easy to interpret and visualize.\n",
        "  - Handle both categorical and numerical data.\n",
        "  - Can be used for both classification and regression.\n",
        "\n",
        "- **Limitations**:\n",
        "  - Prone to overfitting, especially with complex trees.\n",
        "  - Can be unstable (small changes in the data can lead to different tree structures).\n"
      ],
      "metadata": {
        "id": "4LiI-yC7RrhZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Write a Python program to:\n",
        " - Load the Iris Dataset\n",
        " - Train a Decision Tree Classifier using the Gini criterion\n",
        " - Print the model’s accuracy and feature importances"
      ],
      "metadata": {
        "id": "j-dZkQ6nSu5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier using Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\", clf.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCWP3CXgStir",
        "outputId": "95e5aeff-590c-4e04-b4b5-41d237a357af"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n",
            "Feature Importances: [0.         0.01911002 0.89326355 0.08762643]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write a Python program to:\n",
        "   - Load the Iris Dataset\n",
        "   - Train a Decision Tree Classifier with max_depth=3 and compare its accuracy toa fully-grown tree."
      ],
      "metadata": {
        "id": "4kU-M0Y3TFCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Decision Tree with max_depth=3\n",
        "clf_pruned = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_pruned.fit(X_train, y_train)\n",
        "\n",
        "# Train a fully-grown Decision Tree\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "\n",
        "# Compare accuracies\n",
        "accuracy_pruned = clf_pruned.score(X_test, y_test)\n",
        "accuracy_full = clf_full.score(X_test, y_test)\n",
        "\n",
        "print(f\"Accuracy with max_depth=3: {accuracy_pruned:.2f}\")\n",
        "print(f\"Accuracy with fully-grown tree: {accuracy_full:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2xdebnZTXdN",
        "outputId": "51d9398c-fb7a-4638-e022-fb64241db161"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 1.00\n",
            "Accuracy with fully-grown tree: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Write a Python program to:\n",
        "    - Load the Boston Housing Dataset\n",
        "    - Train a Decision Tree Regressor\n",
        "    - Print the Mean Squared Error (MSE) and feature importances"
      ],
      "metadata": {
        "id": "vqlYQd7TTkyF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CAu-KAC5S6Ps"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OSZKYcR2Rkxr",
        "outputId": "b6187dfd-864d-4fae-9c1a-3b310645b62b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2270687920.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_boston\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecisionTreeRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load the Boston Housing dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/datasets/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \"\"\"\n\u001b[1;32m    160\u001b[0m         )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the Boston Housing dataset\n",
        "boston = load_boston()\n",
        "X_boston = boston.data\n",
        "y_boston = boston.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train_boston, X_test_boston, y_train_boston, y_test_boston = train_test_split(X_boston, y_boston, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train_boston, y_train_boston)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = regressor.predict(X_test_boston)\n",
        "\n",
        "# Calculate MSE\n",
        "mse = mean_squared_error(y_test_boston, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\", regressor.feature_importances_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution with California housing data set"
      ],
      "metadata": {
        "id": "xdXZMZqbUW1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the California Housing dataset\n",
        "california = fetch_california_housing()\n",
        "X_california = california.data\n",
        "y_california = california.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train_california, X_test_california, y_train_california, y_test_california = train_test_split(X_california, y_california, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train_california, y_train_california)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = regressor.predict(X_test_california)\n",
        "\n",
        "# Calculate MSE\n",
        "mse = mean_squared_error(y_test_california, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\", regressor.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTE0KiJwUcf8",
        "outputId": "3966584f-771e-490a-ea73-9f7fda22056b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.53\n",
            "Feature Importances: [0.52345628 0.05213495 0.04941775 0.02497426 0.03220553 0.13901245\n",
            " 0.08999238 0.08880639]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Write a Python program to:\n",
        "    - Load the Iris Dataset\n",
        "    - Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
        "    - Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "URy51mtlUfkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Parameters for GridSearchCV\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Initialize the Decision Tree Classifier\n",
        "clf_grid = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Use GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(clf_grid, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters from the grid search\n",
        "best_params = grid_search.best_params_\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "\n",
        "# Best model and its accuracy\n",
        "best_model = grid_search.best_estimator_\n",
        "accuracy = best_model.score(X_test, y_test)\n",
        "print(f\"Accuracy with best parameters: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z78i3HMYUnGY",
        "outputId": "5448a257-e20c-4c24-cbef-63a6f9ea9063"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 5, 'min_samples_split': 10}\n",
            "Accuracy with best parameters: 1.00\n"
          ]
        }
      ]
    }
  ]
}