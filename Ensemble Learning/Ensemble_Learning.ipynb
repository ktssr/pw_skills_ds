{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Learning"
      ],
      "metadata": {
        "id": "O2LnolNsLD4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "Ensemble Learning is a technique in machine learning where multiple models are combined to solve a problem and improve performance. Instead of relying on a single model, ensemble methods build multiple models and combine their outputs to make final prediction. The key idea is to reduce errors like bias and variance by taking the strength of multiple models.\n",
        "\n",
        "## 2. What is the difference between Bagging and Boosting?\n",
        "Bagging (Bootstrap Aggregating) is a technique where multiple models are trained independently using random samples (with replacement) from the training data. The final output is given by majority vote or average. Boosting is a sequential method where each model tries to correct the errors made by the previous one. Bagging reduces variance and Boosting reduces bias. Bagging builds models in parallel, Boosting builds in sequence.\n",
        "\n",
        "## 3. What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "Bootstrap sampling is a method where random samples are taken from the dataset with replacement. This means some records can appear more than once in the sample. In Bagging methods like Random Forest, bootstrap sampling is used to train each decision tree on a different subset of the data. This helps in creating diverse models and reduces variance.\n",
        "\n",
        "## 4. What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "OOB samples are the data points not selected in a bootstrap sample. Since bootstrap samples are created with replacement, about 1/3rd of data is left out. These left out samples are called Out-of-Bag samples. OOB score is the accuracy calculated using these samples and it acts like a cross-validation score to evaluate performance without needing a separate validation set.\n",
        "\n",
        "## 5. Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        "In a single decision tree, feature importance is calculated based on how much each feature reduces impurity like Gini index or entropy. But since itâ€™s only one tree, the importance might be biased or unstable. In Random Forest, the importance is averaged over many trees, which gives more reliable and stable importance values because it reduces overfitting and takes diverse paths into account.\n"
      ],
      "metadata": {
        "id": "ph2jOp9qLHp2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Write a Python program to:\n",
        "  - Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer()\n",
        "  - Train a Random Forest Classifier\n",
        "  - Print the top 5 most important features based on feature importance scores."
      ],
      "metadata": {
        "id": "ME6LYFdgL5Ke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Train Random Forest\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "importances = pd.Series(model.feature_importances_, index=X.columns)\n",
        "top_5 = importances.sort_values(ascending=False).head(5)\n",
        "print(\"Top 5 Important Features:\")\n",
        "print(top_5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmOq0PsKLGkF",
        "outputId": "c84cdcc9-79d0-42db-b016-35fb1c26ab81"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Important Features:\n",
            "worst area              0.139357\n",
            "worst concave points    0.132225\n",
            "mean concave points     0.107046\n",
            "worst radius            0.082848\n",
            "worst perimeter         0.080850\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.Write a Python program to:\n",
        "  - Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "  - Evaluate its accuracy and compare with a single Decision Tree"
      ],
      "metadata": {
        "id": "m7jP4mCKMOfa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyNHH24ZJSnc",
        "outputId": "d5d66f10-9f32-4d9d-e78e-b1c16ddcc595"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "Bagging Classifier Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Single Decision Tree\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(X_train, y_train)\n",
        "tree_acc = accuracy_score(y_test, tree.predict(X_test))\n",
        "\n",
        "# Bagging Classifier\n",
        "bagging = BaggingClassifier(DecisionTreeClassifier(), random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "bagging_acc = accuracy_score(y_test, bagging.predict(X_test))\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", tree_acc)\n",
        "print(\"Bagging Classifier Accuracy:\", bagging_acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Write a Python program to:\n",
        "  - Train a Random Forest Classifier\n",
        "  - Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "  - Print the best parameters and final accuracy"
      ],
      "metadata": {
        "id": "JXduwO_3MZjA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Load data\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Define model and grid\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [3, 5, None]\n",
        "}\n",
        "\n",
        "# Grid search\n",
        "grid = GridSearchCV(model, param_grid, cv=5)\n",
        "grid.fit(X, y)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jywMH9_QMZP_",
        "outputId": "ee03b8fd-5d25-4786-a0ad-468f0019b6e6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 5, 'n_estimators': 100}\n",
            "Best Accuracy: 0.9596180717279925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.Write a Python program to:\n",
        "  - Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
        "  - Compare their Mean Squared Errors (MSE)"
      ],
      "metadata": {
        "id": "pB9ZZJ7tMrFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, random_state=42)\n",
        "\n",
        "# Bagging Regressor\n",
        "bagging = BaggingRegressor(random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "bagging_pred = bagging.predict(X_test)\n",
        "bagging_mse = mean_squared_error(y_test, bagging_pred)\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_pred = rf.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "print(\"Bagging Regressor MSE:\", bagging_mse)\n",
        "print(\"Random Forest Regressor MSE:\", rf_mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8knjejefM5i4",
        "outputId": "c483c8c5-49ba-4709-dfc1-8730e6cf5647"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.27872374841230696\n",
            "Random Forest Regressor MSE: 0.2542358390056568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data. You decide to use ensemble techniques to increase model performance. Explain your step-by-step approach to:\n",
        "  - Choose between Bagging or Boosting\n",
        "  - Handle overfitting\n",
        "  - Select base models\n",
        "  - Evaluate performance using cross-validation\n",
        "  - Justify how ensemble learning improves decision-making in this real-world context.\n",
        "\n",
        "In this scenario we are predicting loan default using customer demographic and transaction history.\n",
        "\n",
        "### Choose between Bagging or Boosting:\n",
        "I will choose Boosting (like Gradient Boosting or XGBoost) as boosting handles imbalanced data and reduces bias. It gives better performance in classification problems like loan default.\n",
        "\n",
        "### Handle Overfitting:\n",
        "I will use techniques like:\n",
        "Limit tree depth (max_depth)\n",
        "Use regularization parameters (like learning_rate)\n",
        "Early stopping\n",
        "Cross-validation to monitor performance\n",
        "\n",
        "### Select Base Models:\n",
        "I will use Decision Trees as base learners since they perform well as weak learners in ensemble models. I might also try logistic regression with Boosting.\n",
        "\n",
        "### Evaluate performance using cross-validation:\n",
        "I will use stratified k-fold cross-validation to maintain class ratio in each fold. Metrics like ROC-AUC, precision, and recall will be used for imbalanced data.\n",
        "\n",
        "### Justify how ensemble learning improves decision-making:\n",
        "Ensemble models provide more stable and accurate predictions. In loan default prediction, they reduce false positives and false negatives. This improves risk assessment and helps the bank make better loan approval decisions."
      ],
      "metadata": {
        "id": "UBzDHpLUNCBo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0K8rpX9EM1uE"
      }
    }
  ]
}