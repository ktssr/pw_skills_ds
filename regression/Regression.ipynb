{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Regression"
      ],
      "metadata": {
        "id": "nybGKzNdGog7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Simple Linear Regression?\n",
        "\n",
        "Simple Linear Regression is a statistical method used to model the relationship between a dependent variable (Y) and a single independent variable (X). The relationship is represented using a straight line with an equation `Y = mX + c`, where `m` is the slope and `c` is the intercept. It is useful when we want to understand how a change in one variable affects another.\n",
        "\n",
        "\n",
        "\n",
        "## What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "The key assumptions of Simple Linear Regression include:\n",
        "- **Linearity**: The relationship between X and Y is linear.\n",
        "- **Independence**: The residuals (errors) are independent.\n",
        "- **Homoscedasticity**: The residuals have constant variance.\n",
        "- **Normality**: The residuals are normally distributed.\n",
        "\n",
        "\n",
        "\n",
        "## What does the coefficient `m` represent in the equation `Y = mX + c`?\n",
        "\n",
        "The coefficient `m` represents the slope of the line. It indicates the change in the dependent variable `Y` for every one unit increase in the independent variable `X`.\n",
        "\n",
        "\n",
        "\n",
        "## What does the intercept `c` represent in the equation `Y = mX + c`?\n",
        "\n",
        "The intercept `c` is the point where the regression line crosses the Y-axis. It represents the value of `Y` when `X` is zero.\n",
        "\n",
        "\n",
        "\n",
        "## How do we calculate the slope `m` in Simple Linear Regression?\n",
        "\n",
        "The slope `m` is calculated using the formula:\n",
        "\n",
        "`m = sum((Xi - X_mean) * (Yi - Y_mean)) / sum((Xi - X_mean)^2)`\n",
        "\n",
        "This formula finds the line that minimizes the squared differences between the predicted and actual values.\n",
        "\n",
        "\n",
        "\n",
        "## What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        "The least squares method is used to find the best-fitting line by minimizing the sum of the squared differences between the observed values and the predicted values. It ensures that the total error is as small as possible.\n",
        "\n",
        "\n",
        "\n",
        "## How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "\n",
        "R² measures how well the independent variable explains the variation in the dependent variable. A value close to 1 means the model explains most of the variation, while a value near 0 means it explains very little.\n",
        "\n",
        "\n",
        "\n",
        "## What is Multiple Linear Regression?\n",
        "\n",
        "Multiple Linear Regression is an extension of Simple Linear Regression where two or more independent variables are used to predict the value of a dependent variable. It helps in modeling more complex relationships where the outcome depends on multiple factors.\n",
        "\n",
        "\n",
        "\n",
        "## What is the main difference between Simple and Multiple Linear Regression?\n",
        "\n",
        "The main difference is in the number of independent variables. Simple Linear Regression uses one independent variable, whereas Multiple Linear Regression uses two or more. This allows Multiple Linear Regression to model more complex real-world scenarios.\n",
        "\n",
        "\n",
        "\n",
        "## What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        "The assumptions are similar to those in Simple Linear Regression but also include:\n",
        "- **Linearity**: Relationship between independent and dependent variables is linear.\n",
        "- **Independence**: Observations are independent of each other.\n",
        "- **Homoscedasticity**: Residuals have constant variance.\n",
        "- **Normality**: Residuals are normally distributed.\n",
        "- **No Multicollinearity**: Independent variables should not be highly correlated with each other.\n",
        "\n",
        "\n",
        "\n",
        "## What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "Heteroscedasticity refers to the condition where the variance of residuals is not constant across all levels of the independent variables. It can lead to inefficient estimates and unreliable hypothesis tests, which may affect the accuracy and trustworthiness of the model.\n",
        "\n",
        "\n",
        "\n",
        "## How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "To improve a model with multicollinearity, you can:\n",
        "- Remove or combine highly correlated independent variables.\n",
        "- Use dimensionality reduction techniques like PCA.\n",
        "- Apply regularization techniques such as Ridge or Lasso regression.\n",
        "\n",
        "\n",
        "\n",
        "## What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "Categorical variables can be transformed using:\n",
        "- **One-hot encoding**: Converts categories into binary columns (0 or 1).\n",
        "- **Label encoding**: Assigns numeric values to categories (useful for ordinal data).\n",
        "- **Dummy variables**: Similar to one-hot but excludes one column to avoid multicollinearity.\n",
        "\n",
        "\n",
        "\n",
        "## What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "Interaction terms are used to model the combined effect of two or more variables on the dependent variable. They help in capturing relationships where the effect of one variable depends on the value of another.\n",
        "\n",
        "\n",
        "\n",
        "## How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        "In Simple Linear Regression, the intercept represents the expected value of the dependent variable when the independent variable is zero. In Multiple Linear Regression, it represents the expected value of the dependent variable when **all** independent variables are zero, which might not always be meaningful in real-world context.\n",
        "\n",
        "\n",
        "\n",
        "## What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        "The slope shows the rate of change in the dependent variable for a unit change in the independent variable. It directly affects predictions because it determines the direction and strength of the relationship.\n",
        "\n",
        "\n",
        "\n",
        "## How does the intercept in a regression model provide context for the relationship between variables?\n",
        "\n",
        "The intercept provides a baseline value of the dependent variable when all independent variables are zero. It gives context to the regression line and helps anchor predictions, although in some cases it may not be interpretable.\n",
        "\n",
        "\n",
        "\n",
        "## What are the limitations of using R² as a sole measure of model performance?\n",
        "\n",
        "R² shows how well the model explains the variance in the data, but it doesn't indicate if the model is appropriate or if variables are significant. A high R² can result from overfitting, especially in models with many predictors. Adjusted R² and other metrics like RMSE should also be considered.\n",
        "\n",
        "\n",
        "\n",
        "## How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "A large standard error suggests that the estimated coefficient is not precise. It implies that the actual value of the coefficient could vary significantly, reducing the confidence in its effect on the dependent variable.\n",
        "\n",
        "\n",
        "\n",
        "## How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "\n",
        "Heteroscedasticity can be identified by plotting residuals vs. predicted values. If the spread of residuals increases or decreases instead of remaining constant, it indicates heteroscedasticity. Addressing it ensures more reliable coefficient estimates and valid statistical inferences.\n",
        "\n",
        "\n",
        "\n",
        "## What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "\n",
        "It means that some of the predictors may not be contributing meaningfully to the model. Adjusted R² penalizes the addition of irrelevant variables, so a large gap between R² and adjusted R² can indicate overfitting.\n",
        "\n",
        "\n",
        "\n",
        "## Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        "Scaling ensures that all variables contribute equally to the model, especially when regularization is applied (like Ridge or Lasso). It also helps in interpreting coefficients more consistently when units of measurement vary widely.\n",
        "\n",
        "\n",
        "\n",
        "## What is polynomial regression?\n",
        "\n",
        "Polynomial regression is a form of regression analysis where the relationship between the independent variable and the dependent variable is modeled as an nth-degree polynomial. It is used when the data shows a non-linear trend.\n",
        "\n",
        "\n",
        "\n",
        "## How does polynomial regression differ from linear regression?\n",
        "\n",
        "Linear regression models a straight-line relationship, while polynomial regression allows for curves by introducing powers of the independent variable (e.g., x², x³). This provides flexibility to fit more complex patterns in the data.\n",
        "\n",
        "\n",
        "\n",
        "## When is polynomial regression used?\n",
        "\n",
        "Polynomial regression is used when the data shows a curved or non-linear relationship, and a straight line doesn’t fit well. For example, modeling population growth, price-demand curves, or trajectories.\n",
        "\n",
        "\n",
        "\n",
        "## What is the general equation for polynomial regression?\n",
        "\n",
        "The general equation is:\n",
        "\n",
        "`Y = b0 + b1*X + b2*X² + b3*X³ + ... + bn*Xⁿ`\n",
        "\n",
        "Where `n` is the degree of the polynomial, and `b0`, `b1`, ..., `bn` are the coefficients.\n",
        "\n",
        "\n",
        "\n",
        "## Can polynomial regression be applied to multiple variables?\n",
        "\n",
        "Yes, polynomial regression can be extended to multiple variables by including interaction and power terms for each variable. However, it increases the complexity and can lead to overfitting if not handled carefully.\n",
        "\n",
        "\n",
        "\n",
        "## What are the limitations of polynomial regression?\n",
        "\n",
        "- Risk of overfitting, especially with high-degree polynomials\n",
        "- Difficult to interpret coefficients\n",
        "- Sensitive to outliers\n",
        "- May not generalize well to unseen data\n",
        "\n",
        "\n",
        "\n",
        "## What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "\n",
        "We can use:\n",
        "- **Cross-validation**\n",
        "- **Adjusted R²**\n",
        "- **AIC/BIC**\n",
        "- **Residual plots**\n",
        "- **Validation error on a test set**\n",
        "\n",
        "These help ensure the model fits well without overfitting.\n",
        "\n",
        "\n",
        "\n",
        "## Why is visualization important in polynomial regression?\n",
        "\n",
        "Visualization helps in understanding the shape of the fitted curve and identifying whether the model is underfitting or overfitting. It provides intuitive insights into how well the model captures the pattern in the data.\n",
        "\n",
        "\n",
        "\n",
        "## How is polynomial regression implemented in Python?\n",
        "\n",
        "Polynomial regression can be implemented using `scikit-learn`:\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
        "model.fit(X, y)\n",
        "```\n",
        "This allows transforming the features and fitting the polynomial model in a single pipeline."
      ],
      "metadata": {
        "id": "BDna8V_JGsHB"
      }
    }
  ]
}